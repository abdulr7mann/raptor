import logging
import re

from api_pentest.core.models import ScenarioApplicability, Severity, TestStatus
from api_pentest.scenarios.base_scenario import BaseScenario

logger = logging.getLogger(__name__)


class S08APIResponses(BaseScenario):
    """S08 - API Response Data Exposure detection."""

    SCENARIO_ID = "S08"
    SCENARIO_NAME = "API Response Data Exposure"
    OWASP_ID = "API3:2023"
    OWASP_NAME = "Broken Object Property Level Authorization"
    APPLICABILITY = ScenarioApplicability(
        classifications=["public", "protected"],  # Exclude auth-endpoint (returns creds by design)
    )

    SENSITIVE_FIELD_PATTERNS = re.compile(
        r'"(password|passwd|pwd|secret|api_key|apikey|api_secret|'
        r'access_token|refresh_token|private_key|credit_card|'
        r'card_number|cvv|cvc|ssn|social_security|'
        r'bank_account|routing_number|'
        r'auth_token|session_id|session_token|'
        r'secret_key|encryption_key|'
        r'otp|mfa_secret|totp_secret)"\s*:',
        re.IGNORECASE,
    )

    SENSITIVE_DATA_PATTERNS = re.compile(
        r'(\b\d{3}-\d{2}-\d{4}\b|'  # SSN
        r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b|'  # Credit card
        r'\b[A-Za-z0-9+/]{40,}={0,2}\b)',  # Long base64 (potential keys)
        re.IGNORECASE,
    )

    VERBOSE_ERROR_PATTERNS = re.compile(
        r'(Traceback \(most recent|at [\w.]+\([\w.]+:\d+\)|'
        r'Exception in thread|java\.lang\.|'
        r'System\.NullReferenceException|'
        r'Fatal error:|PHP Fatal|PHP Warning|'
        r'SQLSTATE\[|mysql_|pg_query|sqlite3_|'
        r'/usr/|/var/|/home/|/opt/|C:\\\\|'
        r'stack trace|internal server error.*detail|'
        r'debug.*true|verbose.*error)',
        re.IGNORECASE,
    )

    INFO_LEAK_HEADERS = [
        "Server",
        "X-Powered-By",
        "X-AspNet-Version",
        "X-Debug-Token",
        "X-Debug-Token-Link",
        "X-Runtime",
        "X-Request-Id",
    ]

    # Fields that are expected in auth-endpoint responses (login, token, etc.)
    EXPECTED_AUTH_FIELDS = {
        "auth_token", "access_token", "refresh_token",
        "token", "session_token", "session_id",
    }

    # Mass Assignment: fields attackers try to inject to gain privileges
    MASS_ASSIGNMENT_PAYLOADS = [
        # Admin/role escalation
        ("admin", True),
        ("admin", "true"),
        ("is_admin", True),
        ("isAdmin", True),
        ("role", "admin"),
        ("role", "administrator"),
        ("roles", ["admin"]),
        ("user_type", "admin"),
        ("userType", "admin"),
        ("type", "admin"),
        ("access_level", 999),
        ("accessLevel", "admin"),
        ("permissions", ["*"]),
        ("privilege", "admin"),
        ("is_superuser", True),
        ("superuser", True),
        # Account status manipulation
        ("verified", True),
        ("is_verified", True),
        ("email_verified", True),
        ("active", True),
        ("is_active", True),
        ("enabled", True),
        ("approved", True),
        ("status", "active"),
        ("account_status", "verified"),
        # Subscription/plan escalation
        ("plan", "premium"),
        ("subscription", "enterprise"),
        ("tier", "unlimited"),
        ("credits", 999999),
        ("balance", 999999),
    ]

    def get_test_cases(self) -> list[str]:
        cases = [
            "sensitive_field_exposure",
            "verbose_error_detection",
            "response_header_info_leak",
            "mass_assignment",
        ]
        if self.oauth and self.oauth_b:
            cases.append("cross_role_field_comparison")
        return cases

    def execute_test(self, test_name: str):
        if test_name == "sensitive_field_exposure":
            self._test_sensitive_field_exposure()
        elif test_name == "verbose_error_detection":
            self._test_verbose_error_detection()
        elif test_name == "response_header_info_leak":
            self._test_response_header_info_leak()
        elif test_name == "cross_role_field_comparison":
            self._test_cross_role_field_comparison()
        elif test_name == "mass_assignment":
            self._test_mass_assignment()

    def _test_sensitive_field_exposure(self):
        """Check if API responses contain sensitive data fields."""
        token = self.get_token_a()
        sensitive_found = False

        for ep in self.endpoints:
            evidence = self.make_request(ep, token=token)

            if not self.is_success_status(evidence.response_status):
                continue

            body = evidence.response_body

            # Check for sensitive field names
            field_matches = self.SENSITIVE_FIELD_PATTERNS.findall(body)
            if field_matches:
                # Filter out expected auth fields for auth-endpoints
                if self.is_auth_endpoint(ep):
                    field_matches = [
                        f for f in field_matches
                        if f.lower() not in self.EXPECTED_AUTH_FIELDS
                    ]
                    if not field_matches:
                        logger.debug(
                            "Skipping sensitive field finding for auth-endpoint %s %s: "
                            "fields are expected auth response (%s)",
                            ep.method, ep.url, ep.classification_reason,
                        )

                if field_matches:
                    unique_fields = list(set(field_matches))
                    sensitive_found = True
                    self.add_result(
                        "sensitive_field_exposure",
                        TestStatus.FAIL,
                        f"Sensitive fields in response: {', '.join(unique_fields)} at {ep.method} {ep.url}",
                        endpoint_name=ep.full_name,
                        evidence=evidence,
                    )
                    self.log_finding(
                        severity=Severity.HIGH,
                        title=f"Sensitive data fields exposed in {ep.method} {ep.url}",
                        description=(
                            f"Response contains sensitive field(s): {', '.join(unique_fields)}. "
                            "These fields should be filtered from API responses."
                        ),
                        endpoint=f"{ep.method} {ep.url}",
                        evidence=evidence,
                        remediation=(
                            "Use response DTOs/serializers to explicitly whitelist returned fields. "
                            "Never return sensitive fields like passwords, tokens, or keys."
                        ),
                    )

            # Check for sensitive data patterns
            data_matches = self.SENSITIVE_DATA_PATTERNS.findall(body)
            if data_matches:
                sensitive_found = True
                self.log_finding(
                    severity=Severity.MEDIUM,
                    title=f"Potential sensitive data in {ep.method} {ep.url}",
                    description=(
                        f"Response may contain sensitive data patterns "
                        f"(SSN, credit card, or encoded keys)."
                    ),
                    endpoint=f"{ep.method} {ep.url}",
                    evidence=evidence,
                    remediation="Review data returned by this endpoint. Mask or remove sensitive values.",
                )

        if not sensitive_found:
            self.add_result(
                "sensitive_field_exposure",
                TestStatus.PASS,
                "No sensitive fields detected in responses",
            )

    def _test_verbose_error_detection(self):
        """Check if error responses expose internal details."""
        token = self.get_token_a()
        verbose_found = False

        # Trigger errors with bad input
        error_triggers = [
            {"override_url_suffix": "/nonexistent-path-12345"},
            {"override_body": "not-json-{{{"},
            {"override_body": {"id": "'; DROP TABLE--"}},
            {"override_method": "INVALID"},
        ]

        for ep in self.endpoints[:10]:
            # Normal error (bad path)
            from urllib.parse import urlparse
            parsed = urlparse(ep.url)
            bad_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}/nonexistent-12345"
            evidence = self.make_request(ep, token=token, override_url=bad_url)

            if evidence.response_body:
                matches = self.VERBOSE_ERROR_PATTERNS.findall(evidence.response_body)
                if matches:
                    verbose_found = True
                    self.add_result(
                        "verbose_error_detection",
                        TestStatus.FAIL,
                        f"Verbose error at {bad_url}: {matches[0][:80]}",
                        endpoint_name=ep.full_name,
                        evidence=evidence,
                    )
                    self.log_finding(
                        severity=Severity.MEDIUM,
                        title="Verbose error message exposes internal details",
                        description=(
                            f"Error response from {bad_url} contains internal details: "
                            f"'{matches[0][:100]}'. This may reveal stack traces, file paths, "
                            "or database information."
                        ),
                        endpoint=f"GET {bad_url}",
                        evidence=evidence,
                        remediation=(
                            "Return generic error messages in production. "
                            "Log detailed errors server-side only."
                        ),
                    )

            # Also try sending bad body to POST/PUT endpoints
            if ep.method.upper() in ("POST", "PUT", "PATCH"):
                evidence = self.make_request(
                    ep, token=token, override_body="<<<INVALID JSON>>>"
                )
                if evidence.response_body:
                    matches = self.VERBOSE_ERROR_PATTERNS.findall(evidence.response_body)
                    if matches:
                        verbose_found = True
                        self.log_finding(
                            severity=Severity.MEDIUM,
                            title=f"Verbose error on malformed body at {ep.method} {ep.url}",
                            description=f"Sending invalid body triggered detailed error: '{matches[0][:100]}'",
                            endpoint=f"{ep.method} {ep.url}",
                            evidence=evidence,
                            remediation="Validate request body format and return generic 400 errors.",
                        )

        if not verbose_found:
            self.add_result(
                "verbose_error_detection",
                TestStatus.PASS,
                "No verbose error messages detected",
            )

    def _test_response_header_info_leak(self):
        """Check for information leakage in response headers."""
        token = self.get_token_a()
        ep = self.endpoints[0] if self.endpoints else None
        if not ep:
            self.add_result("response_header_info_leak", TestStatus.SKIP, "No endpoints")
            return

        evidence = self.make_request(ep, token=token)
        leaks = []

        for header_name in self.INFO_LEAK_HEADERS:
            # Case-insensitive header lookup
            for resp_header, value in evidence.response_headers.items():
                if resp_header.lower() == header_name.lower():
                    leaks.append(f"{resp_header}: {value}")

        if leaks:
            self.add_result(
                "response_header_info_leak",
                TestStatus.FAIL,
                f"Info leak headers found: {'; '.join(leaks)}",
                evidence=evidence,
            )
            self.log_finding(
                severity=Severity.LOW,
                title="Information leakage via response headers",
                description=(
                    f"The following headers may reveal server technology details: "
                    f"{'; '.join(leaks)}"
                ),
                endpoint=f"{ep.method} {ep.url}",
                evidence=evidence,
                remediation=(
                    "Remove or customize Server, X-Powered-By, and debug headers "
                    "in production environments."
                ),
            )
        else:
            self.add_result(
                "response_header_info_leak",
                TestStatus.PASS,
                "No information leakage headers detected",
                evidence=evidence,
            )

    def _test_cross_role_field_comparison(self):
        """Compare response fields between User A and User B to detect over-exposure."""
        token_a = self.get_token_a()
        token_b = self.get_token_b()

        if not token_a or not token_b:
            self.add_result(
                "cross_role_field_comparison",
                TestStatus.SKIP,
                "Need both User A and B tokens",
            )
            return

        over_exposure = False
        tested = 0

        for ep in self.endpoints[:10]:
            if ep.method.upper() != "GET":
                continue

            resp_a = self.make_request(ep, token=token_a)
            resp_b = self.make_request(ep, token=token_b)

            if not (
                self.is_success_status(resp_a.response_status)
                and self.is_success_status(resp_b.response_status)
            ):
                continue

            tested += 1

            # Compare response field sets using parse_json_safe
            data_a = self.parse_json_safe(resp_a)
            data_b = self.parse_json_safe(resp_b)

            if data_a is None or data_b is None:
                continue

            fields_a = self._extract_field_names(data_a)
            fields_b = self._extract_field_names(data_b)

            extra_in_a = fields_a - fields_b
            extra_in_b = fields_b - fields_a

            if extra_in_a or extra_in_b:
                over_exposure = True
                details = []
                if extra_in_a:
                    details.append(f"Extra in User A: {', '.join(sorted(extra_in_a)[:10])}")
                if extra_in_b:
                    details.append(f"Extra in User B: {', '.join(sorted(extra_in_b)[:10])}")

                self.add_result(
                    "cross_role_field_comparison",
                    TestStatus.FAIL,
                    f"Field difference at {ep.method} {ep.url}: {'; '.join(details)}",
                    endpoint_name=ep.full_name,
                )
                self.log_finding(
                    severity=Severity.MEDIUM,
                    title=f"Different field sets returned for different users at {ep.url}",
                    description=(
                        f"GET {ep.url} returns different fields for User A vs User B. "
                        f"{'; '.join(details)}. This may indicate role-based field filtering, "
                        "or it may reveal data that should be restricted."
                    ),
                    endpoint=f"{ep.method} {ep.url}",
                    remediation="Ensure response field filtering is intentional and based on user permissions.",
                )

        if not over_exposure:
            self.add_result(
                "cross_role_field_comparison",
                TestStatus.PASS,
                f"Consistent field sets across roles ({tested} endpoints compared)",
            )

    def _test_mass_assignment(self):
        """Test for Mass Assignment vulnerabilities in creation/update endpoints.

        Mass Assignment occurs when an API accepts more fields than intended,
        allowing attackers to escalate privileges (e.g., admin: true) or
        modify protected fields (e.g., user_id, created_at).
        """
        token = self.get_token_a()
        vuln_found = False

        # Focus on POST (create) and PUT/PATCH (update) endpoints
        mutation_eps = [
            ep for ep in self.endpoints
            if ep.method.upper() in ("POST", "PUT", "PATCH")
            and ep.body
            and isinstance(ep.body, dict)
        ]

        if not mutation_eps:
            self.add_result(
                "mass_assignment",
                TestStatus.SKIP,
                "No POST/PUT/PATCH endpoints with body found",
            )
            return

        for ep in mutation_eps:
            # Get baseline request
            baseline = self.make_request(ep, token=token)

            for field_name, field_value in self.MASS_ASSIGNMENT_PAYLOADS:
                # Skip if field already exists in the original body
                if field_name in ep.body:
                    continue

                # Add undocumented field to body
                modified_body = dict(ep.body)
                modified_body[field_name] = field_value

                evidence = self.make_request(ep, token=token, override_body=modified_body)

                # Check if the field was accepted
                if not self.is_success_status(evidence.response_status):
                    continue

                # Parse response to check if field appears in response
                response_data = self.parse_json_safe(evidence)
                if response_data is None:
                    continue

                # Check if the injected field appears in the response
                field_in_response = self._check_field_in_response(
                    response_data, field_name, field_value
                )

                if field_in_response:
                    vuln_found = True
                    self.add_result(
                        "mass_assignment",
                        TestStatus.FAIL,
                        f"Mass Assignment: {ep.method} {ep.url} accepted '{field_name}={field_value}'",
                        endpoint_name=ep.full_name,
                        evidence=evidence,
                    )
                    self.log_finding(
                        severity=Severity.HIGH,
                        title=f"Mass Assignment vulnerability in {ep.method} {ep.url}",
                        description=(
                            f"Endpoint accepted undocumented field '{field_name}' with value "
                            f"'{field_value}'. This may allow privilege escalation or "
                            "modification of protected attributes."
                        ),
                        endpoint=f"{ep.method} {ep.url}",
                        evidence=evidence,
                        remediation=(
                            "Use explicit allowlists for accepted fields. "
                            "Never directly bind request data to model objects. "
                            "Use DTOs or form objects with defined fields."
                        ),
                    )
                    # Don't test more payloads for this endpoint
                    break

        if not vuln_found:
            self.add_result(
                "mass_assignment",
                TestStatus.PASS,
                f"No Mass Assignment detected ({len(mutation_eps)} endpoints tested)",
            )

    def _check_field_in_response(self, data, field_name: str, field_value) -> bool:
        """Check if a field with matching value exists in response data."""
        if isinstance(data, dict):
            # Check direct field match
            for key, value in data.items():
                if key.lower() == field_name.lower():
                    # Value match (loose comparison for bool/str)
                    if self._values_match(value, field_value):
                        return True
                # Recurse into nested dicts
                if isinstance(value, (dict, list)):
                    if self._check_field_in_response(value, field_name, field_value):
                        return True
        elif isinstance(data, list):
            for item in data:
                if self._check_field_in_response(item, field_name, field_value):
                    return True
        return False

    def _values_match(self, response_val, injected_val) -> bool:
        """Loosely compare response value with injected value."""
        # Direct match
        if response_val == injected_val:
            return True
        # Bool/string equivalence
        if isinstance(injected_val, bool):
            if response_val in (True, "true", "True", 1, "1"):
                return injected_val is True
            if response_val in (False, "false", "False", 0, "0"):
                return injected_val is False
        # String comparison
        if str(response_val).lower() == str(injected_val).lower():
            return True
        return False

    def _extract_field_names(self, data, prefix: str = "") -> set[str]:
        """Recursively extract all field names from a JSON structure."""
        fields = set()
        if isinstance(data, dict):
            for key, value in data.items():
                full_key = f"{prefix}.{key}" if prefix else key
                fields.add(full_key)
                fields.update(self._extract_field_names(value, full_key))
        elif isinstance(data, list) and data:
            fields.update(self._extract_field_names(data[0], prefix))
        return fields
