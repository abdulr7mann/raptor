import logging
import time
from typing import Any

from colorama import Fore, Style, init as colorama_init

from api_pentest.core.api_discovery import ApiProfiler, ArchitectureType
from api_pentest.core.endpoint_classifier import EndpointClassifier
from api_pentest.core.finding_validator import FindingValidator
from api_pentest.core.http_client import PentestHttpClient
from api_pentest.core.input_detector import InputDetector
from api_pentest.core.models import Endpoint, Finding, InputFormat, ScenarioApplicability, Severity, TestResult, TestStatus
from api_pentest.core.oauth2_handler import OAuth2Handler
from api_pentest.core.prerequisite_detector import PrerequisiteChecker
from api_pentest.core.relevance import RelevanceCalculator
from api_pentest.core.response_patterns import ResponsePatternLearner
from api_pentest.reporting.report_generator import ReportGenerator

logger = logging.getLogger(__name__)

SCENARIO_MODULES = {
    "s01": ("api_pentest.scenarios.s01_token_reuse", "S01TokenReuse"),
    "s02": ("api_pentest.scenarios.s02_rate_limiting", "S02RateLimiting"),
    "s03": ("api_pentest.scenarios.s03_idor", "S03IDOR"),
    "s04": ("api_pentest.scenarios.s04_injection", "S04Injection"),
    "s05": ("api_pentest.scenarios.s05_auth_hijacking", "S05AuthHijacking"),
    "s06": ("api_pentest.scenarios.s06_privileged_access", "S06PrivilegedAccess"),
    "s07": ("api_pentest.scenarios.s07_access_controls", "S07AccessControls"),
    "s08": ("api_pentest.scenarios.s08_api_responses", "S08APIResponses"),
    "s09": ("api_pentest.scenarios.s09_business_flow", "S09BusinessFlow"),
    "s10": ("api_pentest.scenarios.s10_ssrf", "S10SSRF"),
    "s11": ("api_pentest.scenarios.s11_security_misconfig", "S11SecurityMisconfig"),
    "s12": ("api_pentest.scenarios.s12_inventory_management", "S12InventoryManagement"),
    "s13": ("api_pentest.scenarios.s13_unsafe_consumption", "S13UnsafeConsumption"),
}


def deduplicate_findings(findings: list[Finding]) -> list[Finding]:
    """Remove duplicate findings. Keep first occurrence, silently drop rest.

    Uniqueness key: (title, endpoint). Endpoint includes HTTP method.
    Different parameters produce different titles, so are naturally distinct.
    """
    seen: set[tuple[str, str]] = set()
    unique: list[Finding] = []
    for finding in findings:
        key = (finding.title, finding.endpoint)
        if key not in seen:
            seen.add(key)
            unique.append(finding)
    return unique


class PentestRunner:
    """Orchestrator that parses input, runs scenarios, and generates reports."""

    def __init__(self, config: dict):
        self.config = config
        self.endpoints: list[Endpoint] = []
        self.all_results: list[TestResult] = []
        self.all_findings: list[Finding] = []
        self.oauth_a: OAuth2Handler | None = None
        self.oauth_b: OAuth2Handler | None = None
        self.http: PentestHttpClient | None = None
        self.response_learner: ResponsePatternLearner | None = None
        self.prerequisite_results: dict = {}
        self.api_profile = None
        self.relevance_calculator = None
        self.finding_validator: FindingValidator | None = None

        colorama_init(autoreset=True)

    def parse_input(self) -> list[Endpoint]:
        """Parse the input file (Postman or OpenAPI) into Endpoint objects."""
        input_file = self.config.get("input_file", "")
        env_file = self.config.get("environment_file")
        base_url = self.config.get("base_url")

        detector = InputDetector(
            file_path=input_file,
            environment_path=env_file,
            base_url_override=base_url,
        )
        self.endpoints = detector.parse()

        fmt = detector.format
        print(f"{Fore.CYAN}Input format: {fmt.value if fmt else 'unknown'}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}Endpoints discovered: {len(self.endpoints)}{Style.RESET_ALL}")

        for w in detector.warnings:
            print(f"{Fore.YELLOW}  Warning: {w}{Style.RESET_ALL}")

        return self.endpoints

    def list_endpoints(self):
        """Print all discovered endpoints without running tests."""
        if not self.endpoints:
            self.parse_input()

        print(f"\n{Fore.WHITE}{'#':<4} {'Method':<8} {'Name':<40} {'URL'}{Style.RESET_ALL}")
        print("-" * 100)
        for i, ep in enumerate(self.endpoints, 1):
            method_color = {
                "GET": Fore.GREEN,
                "POST": Fore.BLUE,
                "PUT": Fore.YELLOW,
                "DELETE": Fore.RED,
                "PATCH": Fore.MAGENTA,
            }.get(ep.method.upper(), Fore.WHITE)
            name = ep.full_name[:38] if ep.full_name else ep.name[:38]
            print(f"{i:<4} {method_color}{ep.method.upper():<8}{Style.RESET_ALL} {name:<40} {ep.url}")

        print(f"\nTotal: {len(self.endpoints)} endpoints")

    def init_oauth(self):
        """Initialize OAuth2 handlers from config."""
        oauth_cfg = self.config.get("oauth2", {})
        if oauth_cfg and oauth_cfg.get("token_url"):
            self.oauth_a = OAuth2Handler(
                token_url=oauth_cfg["token_url"],
                client_id=oauth_cfg.get("client_id", ""),
                client_secret=oauth_cfg.get("client_secret", ""),
                grant_type=oauth_cfg.get("grant_type", "client_credentials"),
                username=oauth_cfg.get("username", ""),
                password=oauth_cfg.get("password", ""),
                scopes=oauth_cfg.get("scopes", []),
            )
            print(f"{Fore.CYAN}OAuth2 handler initialized (User A){Style.RESET_ALL}")

        user_b_cfg = self.config.get("user_b", {})
        if user_b_cfg and (user_b_cfg.get("client_id") or user_b_cfg.get("username")):
            self.oauth_b = OAuth2Handler(
                token_url=oauth_cfg.get("token_url", ""),
                client_id=user_b_cfg.get("client_id", ""),
                client_secret=user_b_cfg.get("client_secret", ""),
                grant_type=oauth_cfg.get("grant_type", "client_credentials"),
                username=user_b_cfg.get("username", ""),
                password=user_b_cfg.get("password", ""),
                scopes=oauth_cfg.get("scopes", []),
            )
            print(f"{Fore.CYAN}OAuth2 handler initialized (User B){Style.RESET_ALL}")

    def init_http(self):
        """Initialize HTTP client."""
        self.http = PentestHttpClient(
            timeout=self.config.get("timeout", 30),
            max_retries=self.config.get("max_retries", 0),
            verify_ssl=self.config.get("verify_ssl", False),
        )

    def run(self, scenario_ids: list[str] | None = None):
        """Run selected (or all) attack scenarios."""
        if not self.endpoints:
            self.parse_input()

        self.init_oauth()
        self.init_http()

        # Learn response patterns before running scenarios
        self.response_learner = ResponsePatternLearner(
            http_client=self.http,
            endpoints=self.endpoints,
            oauth_handler=self.oauth_a,
        )
        self.response_learner.learn()

        # Create finding validator with learned baselines
        baselines = self.response_learner.baselines if self.response_learner else {}
        self.finding_validator = FindingValidator(baselines=baselines)

        # Classify endpoints before running scenarios
        classifier = EndpointClassifier(
            endpoints=self.endpoints,
            openapi_spec=self._get_raw_spec(),
            config=self.config,
        )
        classifier.classify_all()

        # Detect security control prerequisites before running scenarios
        prereq_checker = PrerequisiteChecker(
            http_client=self.http,
            endpoints=self.endpoints,
            config=self.config,
            oauth_handler=self.oauth_a,
        )
        self.prerequisite_results = prereq_checker.check_all()

        # Discover API characteristics and build profile
        profiler = ApiProfiler(
            openapi_spec=self._get_raw_spec(),
            http_client=self.http,
            endpoints=self.endpoints,
            config=self.config,
            response_learner=self.response_learner,
            prerequisite_results=self.prerequisite_results,
        )

        # Check for cached profile first
        cached = profiler.load_cached_profile()
        if cached and not profiler.is_stale(cached):
            self.api_profile = cached
            print(f"{Fore.CYAN}API profile loaded from cache{Style.RESET_ALL}")
        else:
            self.api_profile = profiler.discover()
            profiler.save(self.api_profile)
            print(f"{Fore.CYAN}API discovery complete - profile saved{Style.RESET_ALL}")

        # Print discovery summary
        if self.api_profile:
            schemes = self.api_profile.auth_schemes
            arch = self.api_profile.architecture_type
            ep_count = self.api_profile.endpoint_count
            print(f"  Auth: {', '.join(s.get('scheme_type', '?') for s in schemes) if schemes else 'none detected'}")
            print(f"  Architecture: {arch or 'unknown'}")
            print(f"  Endpoints: {ep_count}")

        # Build relevance calculator for test filtering
        relevance_threshold = self.config.get("relevance_threshold", 0.3)
        self.relevance_calculator = RelevanceCalculator(
            api_profile=self.api_profile,
            prerequisite_results=self.prerequisite_results,
            threshold=relevance_threshold,
        )
        print(f"  Relevance threshold: {relevance_threshold}")

        # Determine which scenarios to run
        selected = scenario_ids or self.config.get("scenarios", [])
        if not selected:
            selected = list(SCENARIO_MODULES.keys())

        print(f"\n{Fore.WHITE}{'=' * 60}")
        print(f" API Pentest - Running {len(selected)} scenario(s)")
        print(f"{'=' * 60}{Style.RESET_ALL}\n")

        total_start = time.time()
        skipped_scenarios = []

        for sid in selected:
            sid = sid.lower().strip()
            if sid not in SCENARIO_MODULES:
                print(f"{Fore.RED}Unknown scenario: {sid} (skipping){Style.RESET_ALL}")
                continue

            module_path, class_name = SCENARIO_MODULES[sid]
            try:
                import importlib
                module = importlib.import_module(module_path)
                scenario_class = getattr(module, class_name)
            except (ImportError, AttributeError) as e:
                print(f"{Fore.RED}Failed to load {sid}: {e}{Style.RESET_ALL}")
                continue

            # Check scenario-level applicability against API profile
            applicability = getattr(scenario_class, 'APPLICABILITY', ScenarioApplicability())

            # Architecture filtering: skip if scenario doesn't apply to detected architecture
            if applicability.architectures and self.api_profile:
                try:
                    profile_arch = ArchitectureType(self.api_profile.architecture_type)
                except ValueError:
                    profile_arch = ArchitectureType.UNKNOWN

                if profile_arch not in applicability.architectures:
                    logger.info(
                        "Skipping %s: architecture %s not in %s",
                        sid.upper(),
                        profile_arch.value,
                        [a.value for a in applicability.architectures],
                    )
                    print(f"{Fore.CYAN}[{sid.upper()}] Skipped: architecture {profile_arch.value} not applicable{Style.RESET_ALL}")
                    skipped_scenarios.append((sid, f"architecture {profile_arch.value} not applicable"))
                    continue

            # Per-test relevance filtering (TEST-04)
            # For each endpoint the scenario will test, calculate relevance score and filter
            endpoints_for_scenario = self.endpoints
            if self.relevance_calculator and self.endpoints:
                skipped_tests = []
                relevant_endpoints = []

                for endpoint in self.endpoints:
                    # Calculate relevance score for this test-endpoint pair
                    score = self.relevance_calculator.calculate(applicability, endpoint)

                    if score.total < relevance_threshold:
                        skipped_tests.append((endpoint.url, score.total))
                        logger.info(
                            "Skipping test %s on %s: relevance score %.2f < threshold %.2f",
                            sid.upper(),
                            endpoint.url,
                            score.total,
                            relevance_threshold,
                        )
                    else:
                        relevant_endpoints.append(endpoint)

                # Log skipped tests if any
                if skipped_tests:
                    for path, score_val in skipped_tests[:3]:  # Limit output
                        print(f"  {Fore.YELLOW}[{sid.upper()}] Skipped {path}: score {score_val:.2f} < {relevance_threshold}{Style.RESET_ALL}")
                    if len(skipped_tests) > 3:
                        print(f"  {Fore.YELLOW}  ... and {len(skipped_tests) - 3} more{Style.RESET_ALL}")

                # If no endpoints are relevant, skip the entire scenario
                if not relevant_endpoints:
                    logger.info(
                        "Skipping %s entirely: no endpoints above relevance threshold %.2f",
                        sid.upper(),
                        relevance_threshold,
                    )
                    skipped_scenarios.append((sid, f"no endpoints above threshold {relevance_threshold}"))
                    continue

                # Use filtered endpoints for this scenario
                endpoints_for_scenario = relevant_endpoints

            scenario = scenario_class()
            scenario.setup(
                endpoints=endpoints_for_scenario,
                oauth_handler=self.oauth_a,
                http_client=self.http,
                config=self.config,
                oauth_handler_b=self.oauth_b,
                response_learner=self.response_learner,
                prerequisite_results=self.prerequisite_results,
                api_profile=self.api_profile,
                finding_validator=self.finding_validator,
            )

            print(f"{Fore.WHITE}[{sid.upper()}] {scenario.SCENARIO_NAME}{Style.RESET_ALL}")
            print(f"  OWASP: {scenario.OWASP_ID} - {scenario.OWASP_NAME}")

            results = scenario.run()
            self.all_results.extend(results)
            self.all_findings.extend(scenario.findings)

            # Print scenario summary
            passed = sum(1 for r in results if r.status == TestStatus.PASS)
            failed = sum(1 for r in results if r.status == TestStatus.FAIL)
            errors = sum(1 for r in results if r.status == TestStatus.ERROR)
            skipped = sum(1 for r in results if r.status == TestStatus.SKIP)

            print(f"  {Fore.GREEN}{passed} passed{Style.RESET_ALL}  "
                  f"{Fore.RED}{failed} failed{Style.RESET_ALL}  "
                  f"{Fore.YELLOW}{errors} errors{Style.RESET_ALL}  "
                  f"{Fore.CYAN}{skipped} skipped{Style.RESET_ALL}")

            for finding in scenario.findings:
                severity_color = {
                    Severity.CRITICAL: Fore.RED + Style.BRIGHT,
                    Severity.HIGH: Fore.RED,
                    Severity.MEDIUM: Fore.YELLOW,
                    Severity.LOW: Fore.CYAN,
                    Severity.INFO: Fore.WHITE,
                }.get(finding.severity, Fore.WHITE)
                print(f"  {severity_color}[{finding.severity.value}]{Style.RESET_ALL} {finding.title}")

            print()

        total_elapsed = time.time() - total_start

        # Print skipped scenario summary if any
        if skipped_scenarios:
            print(f"\n{Fore.CYAN}Scenarios skipped due to applicability:{Style.RESET_ALL}")
            for sid, reason in skipped_scenarios:
                print(f"  {sid.upper()}: {reason}")

        # Deduplicate findings (RPT-04)
        self.all_findings = deduplicate_findings(self.all_findings)

        # Print final summary
        self._print_summary(total_elapsed)

        # Generate reports
        self._generate_reports()

    def _print_summary(self, elapsed: float):
        """Print final summary of all findings."""
        print(f"\n{Fore.WHITE}{'=' * 60}")
        print(f" PENTEST SUMMARY")
        print(f"{'=' * 60}{Style.RESET_ALL}")

        total_tests = len(self.all_results)
        total_findings = len(self.all_findings)

        severity_counts = {}
        for f in self.all_findings:
            severity_counts[f.severity] = severity_counts.get(f.severity, 0) + 1

        print(f"\n  Tests run: {total_tests}")
        print(f"  Findings:  {total_findings}")
        print(f"  Duration:  {elapsed:.1f}s")

        if severity_counts:
            print(f"\n  Findings by severity:")
            for sev in [Severity.CRITICAL, Severity.HIGH, Severity.MEDIUM, Severity.LOW, Severity.INFO]:
                count = severity_counts.get(sev, 0)
                if count > 0:
                    color = {
                        Severity.CRITICAL: Fore.RED + Style.BRIGHT,
                        Severity.HIGH: Fore.RED,
                        Severity.MEDIUM: Fore.YELLOW,
                        Severity.LOW: Fore.CYAN,
                        Severity.INFO: Fore.WHITE,
                    }[sev]
                    print(f"    {color}{sev.value:<10} {count}{Style.RESET_ALL}")

        print()

    def _generate_reports(self):
        """Generate JSON and HTML reports."""
        output_dir = self.config.get("output_dir", "./reports")

        try:
            generator = ReportGenerator(output_dir=output_dir)
            json_path = generator.generate_json(self.all_results, self.all_findings)
            html_path = generator.generate_html(self.all_results, self.all_findings)

            print(f"{Fore.GREEN}Reports generated:{Style.RESET_ALL}")
            print(f"  JSON: {json_path}")
            print(f"  HTML: {html_path}")
        except Exception as e:
            print(f"{Fore.RED}Failed to generate reports: {e}{Style.RESET_ALL}")
            logger.error("Report generation failed", exc_info=True)

    def _get_raw_spec(self) -> dict | None:
        """Return the raw OpenAPI/Swagger spec dict if available, or None.

        For Postman collections, returns None (no security metadata).
        Result is cached to avoid re-parsing.
        """
        if hasattr(self, "_raw_spec_cache"):
            return self._raw_spec_cache

        input_file = self.config.get("input_file", "")
        if not input_file:
            self._raw_spec_cache = None
            return None

        try:
            detector = InputDetector(file_path=input_file)
            detector.load()
            fmt = detector.detect_format()
            if fmt in (InputFormat.SWAGGER_20, InputFormat.OPENAPI_30, InputFormat.OPENAPI_31):
                self._raw_spec_cache = detector.data
            else:
                self._raw_spec_cache = None
        except Exception as e:
            logger.debug("Could not load raw spec for classification: %s", e)
            self._raw_spec_cache = None

        return self._raw_spec_cache
