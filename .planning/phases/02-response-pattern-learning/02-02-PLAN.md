---
phase: 02-response-pattern-learning
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - api_pentest/scenarios/s06_privileged_access.py
  - api_pentest/scenarios/s09_business_flow.py
  - api_pentest/scenarios/s13_unsafe_consumption.py
autonomous: false

must_haves:
  truths:
    - "Running a scan against VAmPI produces zero false positives from HTTP 200 + fail body pattern"
    - "S06 admin_endpoint_access correctly identifies HTTP 200 + {'status': 'fail'} as NOT a successful attack"
    - "S09 mass_creation, lifecycle_abuse, duplicate_creation, business_logic, workflow_bypass all use body-aware validation"
    - "S13 content_type_mismatch and null_special use body-aware validation"
    - "True positives (actual vulnerabilities) are NOT suppressed"
  artifacts:
    - path: "api_pentest/scenarios/s06_privileged_access.py"
      provides: "Updated privilege access tests using is_real_success()"
      contains: "is_real_success"
    - path: "api_pentest/scenarios/s09_business_flow.py"
      provides: "Updated business flow tests using is_real_success()"
      contains: "is_real_success"
    - path: "api_pentest/scenarios/s13_unsafe_consumption.py"
      provides: "Updated unsafe consumption tests using is_real_success()"
      contains: "is_real_success"
  key_links:
    - from: "api_pentest/scenarios/s06_privileged_access.py"
      to: "api_pentest/scenarios/base_scenario.py"
      via: "self.is_real_success(evidence) calls inherited method"
      pattern: "self\\.is_real_success\\(evidence"
    - from: "api_pentest/scenarios/s09_business_flow.py"
      to: "api_pentest/scenarios/base_scenario.py"
      via: "self.is_real_success(evidence) calls inherited method"
      pattern: "self\\.is_real_success\\(evidence"
    - from: "api_pentest/scenarios/s13_unsafe_consumption.py"
      to: "api_pentest/scenarios/base_scenario.py"
      via: "self.is_real_success(evidence) calls inherited method"
      pattern: "self\\.is_real_success\\(evidence"
---

<objective>
Replace is_success_status() with is_real_success() in S06, S09, and S13 scenarios -- the three scenarios producing 10 confirmed false positives against VAmPI from HTTP 200 + fail body pattern.

Purpose: This is the last mile that connects the learned patterns to actual finding decisions. Without this, the learner exists but no scenario uses it, and false positives persist.

Output: Three modified scenario files that use body-aware success validation instead of HTTP-status-only checks.
</objective>

<execution_context>
@/home/abdulr7man/.claude/get-shit-done/workflows/execute-plan.md
@/home/abdulr7man/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-response-pattern-learning/02-RESEARCH.md
@.planning/phases/02-response-pattern-learning/02-01-SUMMARY.md

@api_pentest/scenarios/base_scenario.py
@api_pentest/scenarios/s06_privileged_access.py
@api_pentest/scenarios/s09_business_flow.py
@api_pentest/scenarios/s13_unsafe_consumption.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace is_success_status with is_real_success in S06, S09, S13</name>
  <files>api_pentest/scenarios/s06_privileged_access.py, api_pentest/scenarios/s09_business_flow.py, api_pentest/scenarios/s13_unsafe_consumption.py</files>
  <action>
For each of the three scenario files, replace `self.is_success_status(evidence.response_status)` with `self.is_real_success(evidence)` at SPECIFIC locations where the check determines whether an attack succeeded (i.e., where a finding would be logged). This is a targeted replacement, NOT a global find-replace.

**IMPORTANT DISTINCTION:** There are two categories of is_success_status() calls:
- **Attack validation** (REPLACE): "Did the attack succeed?" -- these produce findings and are the source of false positives. Replace with is_real_success().
- **Baseline/precondition checks** (KEEP): "Did the normal request work?" -- these check if an endpoint is reachable before testing. Keep as is_success_status() because we want HTTP-level success for baselines.

**S06 (s06_privileged_access.py) -- 4 replacements:**

1. `_test_admin_endpoint_access()` line ~80: Replace `if self.is_success_status(evidence.response_status):` with `if self.is_real_success(evidence):` -- this is the attack check (regular user accessing admin endpoint).

2. `_test_horizontal_privilege_escalation()` line ~139: Replace `if self.is_success_status(evidence.response_status):` with `if self.is_real_success(evidence):` -- this is the attack check (User A doing User B's action). KEEP the baseline check on line ~132 (`if not self.is_success_status(baseline.response_status):`) as-is -- that checks if User B can perform the action at all.

3. `_test_privilege_param_escalation()` line ~201: Replace `if self.is_success_status(evidence.response_status):` with `if self.is_real_success(evidence):` -- this is the attack check (escalation params accepted).

4. `_test_service_endpoint_access()` line ~266: Replace `if self.is_success_status(evidence.response_status):` with `if self.is_real_success(evidence):` -- this is the attack check (user accessing service endpoint).

**S09 (s09_business_flow.py) -- 5 replacements:**

1. `_test_mass_creation()` line ~80: Replace `if self.is_success_status(evidence.response_status):` with `if self.is_real_success(evidence):` -- counting successful creations in burst.

2. `_test_lifecycle_abuse()` line ~138: Replace `if self.is_success_status(evidence.response_status):` with `if self.is_real_success(evidence):` -- counting successful lifecycle iterations.

3. `_test_duplicate_creation()` line ~188: Replace BOTH `self.is_success_status(ev1.response_status)` and `self.is_success_status(ev2.response_status)` with `self.is_real_success(ev1)` and `self.is_real_success(ev2)` -- checking if both duplicate requests succeeded at application level.

4. `_test_business_logic()` line ~256: Replace `if self.is_success_status(evidence.response_status):` with `if self.is_real_success(evidence):` -- checking if invalid business data was accepted.

5. `_test_workflow_bypass()` line ~309: Replace `if self.is_success_status(evidence.response_status):` with `if self.is_real_success(evidence):` -- checking if workflow step was bypassed.

**S13 (s13_unsafe_consumption.py) -- 2 replacements (targeted, not all):**

1. `_test_content_type_mismatch()` line ~75: Replace `if self.is_success_status(evidence.response_status):` with `if self.is_real_success(evidence):` -- checking if mismatched content type was accepted.

2. `_test_null_special()` line ~340: Replace `if self.is_success_status(evidence.response_status):` with `if self.is_real_success(evidence):` -- checking if null/special values were accepted.

**DO NOT REPLACE these in S13** (they are different test logic):
- `_test_type_confusion()` line ~144: The `elif self.is_success_status(...)` here follows a check for HTTP 500 server error -- both are legitimate signal types for type confusion testing. Keep as-is.
- `_test_oversized_payload()` lines ~183, ~227: These check `is_success_status(...) or response_status == 500` which is testing whether the server has size limits at all. This is infrastructure testing, not attack validation. Keep as-is.
- `_test_encoding_attacks()` line ~286: Same pattern as type confusion -- `elif is_success_status(...)` after HTTP 500 check. Keep as-is.

The rationale: S13 tests unsafe API consumption patterns where accepting ANY input is the vulnerability. The false positives come specifically from content_type_mismatch and null_special where VAmPI returns 200 + fail body. The other S13 tests look for server errors (500) or specific HTTP status codes, which are valid signals regardless of body content.
  </action>
  <verify>
    python -c "
import ast, sys

# Verify each file has is_real_success calls
for fname, expected_count in [
    ('api_pentest/scenarios/s06_privileged_access.py', 4),
    ('api_pentest/scenarios/s09_business_flow.py', 6),  # 5 locations, but duplicate_creation has 2 calls on one line
    ('api_pentest/scenarios/s13_unsafe_consumption.py', 2),
]:
    with open(fname) as f:
        src = f.read()
    count = src.count('is_real_success')
    print(f'{fname}: {count} is_real_success calls')
    # Also verify is_success_status still exists where needed
    old_count = src.count('is_success_status')
    print(f'  {old_count} is_success_status calls remaining (baseline/precondition checks)')

# Verify all files import OK
import api_pentest.scenarios.s06_privileged_access
import api_pentest.scenarios.s09_business_flow
import api_pentest.scenarios.s13_unsafe_consumption
print('All imports OK')
"
  </verify>
  <done>
    S06: 4 is_success_status calls replaced with is_real_success (1 baseline check retained).
    S09: 5 locations replaced with is_real_success (6 total calls due to duplicate_creation having 2).
    S13: 2 is_success_status calls replaced with is_real_success (4 other calls retained for infrastructure tests).
    All files import without error.
    Baseline/precondition checks preserved as is_success_status.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify false positive elimination against VAmPI</name>
  <what-built>
    Response pattern learning engine (Plan 01) and scenario integration (Task 1 above). The toolkit should now detect HTTP 200 + fail body as application-level failure and suppress false positive findings from S06, S09, and S13.
  </what-built>
  <how-to-verify>
    1. Start VAmPI if not already running (e.g., `docker run -d -p 5000:5000 erev0s/vampi:latest`)
    2. Run targeted scan:
       ```bash
       python run_pentest.py --config vampi_config.yaml --input vampi_openapi.json --scenarios s06,s09,s13
       ```
    3. Verify in scan output:
       - Check for INFO log line: "Learned response patterns for N/M endpoints"
       - Count findings from S06, S09, S13
       - Confirm NONE of the findings come from HTTP 200 + `{"status": "fail"}` responses
       - The 10 previously-reported false positives should be gone
    4. Run full scan to check for regressions:
       ```bash
       python run_pentest.py --config vampi_config.yaml --input vampi_openapi.json
       ```
    5. Verify other scenarios (S03, S04, S07, S08, S10, S12) produce same findings as before (no regressions)
  </how-to-verify>
  <resume-signal>Type "approved" if false positives are eliminated and no regressions, or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
Phase 2 success criteria from ROADMAP.md:
1. "Running a scan against VAmPI produces zero false positives from HTTP 200 + fail body pattern" -- Verified by Task 2 human checkpoint
2. "The toolkit analyzes baseline responses and identifies per-API success/failure indicators before running security tests" -- Verified by Plan 01 (learner runs as pre-pass)
3. "Test validation checks both HTTP status code AND response body structure, not status code alone" -- Verified by is_real_success() replacement in S06/S09/S13
</verification>

<success_criteria>
- S06, S09, S13 use is_real_success() for all attack validation checks
- Baseline/precondition checks still use is_success_status() where appropriate
- VAmPI scan confirms 0 false positives from HTTP 200 + fail body (human-verified)
- No regressions: other scenarios still produce correct results
</success_criteria>

<output>
After completion, create `.planning/phases/02-response-pattern-learning/02-02-SUMMARY.md`
</output>
