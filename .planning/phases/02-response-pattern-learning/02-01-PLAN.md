---
phase: 02-response-pattern-learning
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - api_pentest/core/response_patterns.py
  - api_pentest/runner.py
  - api_pentest/scenarios/base_scenario.py
autonomous: true

must_haves:
  truths:
    - "The toolkit probes endpoints and learns success/failure body patterns before running security tests"
    - "is_real_success() checks both HTTP status code AND response body structure"
    - "When no pattern is learned for an endpoint, behavior falls back to HTTP-status-only (no regression)"
    - "Non-JSON responses fall through gracefully to HTTP status check"
  artifacts:
    - path: "api_pentest/core/response_patterns.py"
      provides: "ResponsePatternLearner class and ResponsePattern dataclass"
      exports: ["ResponsePatternLearner", "ResponsePattern"]
      contains: "class ResponsePatternLearner"
    - path: "api_pentest/runner.py"
      provides: "Pre-pass learning integrated into run() flow"
      contains: "response_learner"
    - path: "api_pentest/scenarios/base_scenario.py"
      provides: "is_real_success() method available to all scenarios"
      contains: "def is_real_success"
  key_links:
    - from: "api_pentest/runner.py"
      to: "api_pentest/core/response_patterns.py"
      via: "ResponsePatternLearner instantiation and learn() call in run()"
      pattern: "ResponsePatternLearner.*learn"
    - from: "api_pentest/runner.py"
      to: "api_pentest/scenarios/base_scenario.py"
      via: "Passing response_learner to scenario.setup()"
      pattern: "response_learner.*setup"
    - from: "api_pentest/scenarios/base_scenario.py"
      to: "api_pentest/core/response_patterns.py"
      via: "is_real_success() delegates to learner for body analysis"
      pattern: "response_learner.*is_real_success|_parse_json"
---

<objective>
Build the ResponsePatternLearner that probes API endpoints before security tests run, learns per-endpoint success/failure body indicators, and exposes is_real_success() on BaseScenario for all scenarios to use.

Purpose: This is the core engine that eliminates HTTP 200 + fail body false positives. Without it, the toolkit cannot distinguish between "HTTP 200 meaning success" and "HTTP 200 with application-level rejection in the body."

Output: New `response_patterns.py` module, modified runner with pre-pass learning, and `is_real_success()` on BaseScenario.
</objective>

<execution_context>
@/home/abdulr7man/.claude/get-shit-done/workflows/execute-plan.md
@/home/abdulr7man/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-response-pattern-learning/02-RESEARCH.md
@.planning/phases/02-response-pattern-learning/02-CONTEXT.md
@.planning/codebase/ARCHITECTURE.md
@.planning/codebase/CONVENTIONS.md

@api_pentest/core/models.py
@api_pentest/scenarios/base_scenario.py
@api_pentest/runner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ResponsePatternLearner with hierarchical signal detection</name>
  <files>api_pentest/core/response_patterns.py</files>
  <action>
Create new file `api_pentest/core/response_patterns.py` containing:

1. **ResponsePattern dataclass** storing learned indicators for a single endpoint:
   - `endpoint_key: str` -- keyed as `"{method}:{url}"` (matching BaseScenario._baselines convention)
   - `status_field: str | None` -- JSON field name if explicit status field detected (e.g., "status")
   - `success_values: set[str]` -- values indicating success (e.g., {"success"})
   - `failure_values: set[str]` -- values indicating failure (e.g., {"fail"})
   - `error_field: str | None` -- field present only in failure responses (e.g., "error")
   - `success_keys: frozenset[str] | None` -- top-level keys in success responses
   - `failure_keys: frozenset[str] | None` -- top-level keys in failure responses

2. **ResponsePatternLearner class** with:
   - `__init__(self, http_client, endpoints, oauth_handler=None)` -- stores references, initializes `self.patterns: dict[str, ResponsePattern]`
   - `learn(self)` -- iterates endpoints, probes each, extracts patterns. For each endpoint:
     - Build endpoint_key as `f"{ep.method}:{ep.url}"`
     - Send a "valid" probe (with auth token if oauth available) to get the success response
     - Send an "invalid" probe (without auth) to get the failure response
     - For GET endpoints: send both probes
     - For POST/PUT/DELETE endpoints: send ONLY the no-auth probe to avoid state mutation (per RESEARCH pitfall 3). Infer success pattern from GET endpoints or from the valid probe only if endpoint is safe.
     - Call `_extract_pattern()` with the two responses
   - `_extract_pattern(self, endpoint_key, success_response, failure_response)` -- compare the two Evidence objects and build a ResponsePattern using hierarchical signal detection:
     - Parse both bodies as JSON (gracefully handle non-JSON)
     - Check STATUS_FIELD_CANDIDATES list: `["status", "success", "ok", "error", "code", "result", "errorCode", "error_code"]`
     - If a candidate field exists in BOTH responses with different values, record it as `status_field` with the respective success/failure values
     - If a candidate field exists ONLY in failure response, record it as `error_field`
     - Record top-level keys difference as structural fingerprint
   - `_parse_json(self, body: str) -> dict | None` -- safely parse JSON, return None for non-JSON
   - `is_real_success(self, evidence: Evidence, endpoint_key: str) -> bool` -- check signals in priority order:
     1. HTTP status must be 200-299 (if not, return False immediately)
     2. Look up learned pattern for endpoint_key
     3. If no pattern learned, return True (fall back to HTTP-status-only -- no regression)
     4. Parse response body as JSON
     5. If JSON and status_field exists: check if value is in failure_values -> return False, in success_values -> return True
     6. If JSON and error_field exists: check if error_field is present in response -> return False
     7. If JSON: compare top-level keys against failure_keys fingerprint -> if exact match to failure_keys and different from success_keys, return False
     8. Default: return True (when in doubt, do NOT suppress -- per RESEARCH pitfall 5)

   Use these constants at module level:
   ```python
   STATUS_FIELD_CANDIDATES = ["status", "success", "ok", "error", "code", "result", "errorCode", "error_code"]
   FAILURE_INDICATORS = {"fail", "failed", "failure", "error", "err", "false", "denied", "unauthorized", "forbidden", "invalid"}
   SUCCESS_INDICATORS = {"success", "successful", "ok", "okay", "true", "completed", "done"}
   ```

   Use `logging.getLogger(__name__)` for logging. Log at INFO level: "Learned response patterns for N/M endpoints". Log at DEBUG level: per-endpoint pattern details (for --verbose mode).

   IMPORTANT: Do NOT import or depend on anything not in stdlib or already in the codebase. Use only: json, logging, dataclasses, and imports from api_pentest.core.models (Evidence, Endpoint) and api_pentest.core.http_client (PentestHttpClient).

   Follow existing codebase conventions:
   - Modern Python 3.10+ type hints (list[T], T | None, dict[K, V])
   - Dataclass with field(default_factory=...) for mutable defaults
   - One logger per module via logging.getLogger(__name__)
   - Class docstrings on all classes
  </action>
  <verify>
    python -c "from api_pentest.core.response_patterns import ResponsePatternLearner, ResponsePattern; print('Import OK')"
  </verify>
  <done>
    ResponsePatternLearner class exists with learn(), is_real_success(), and _extract_pattern() methods.
    ResponsePattern dataclass exists with status_field, success_values, failure_values, error_field, success_keys, failure_keys fields.
    Module imports cleanly with no errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate learner into runner and BaseScenario</name>
  <files>api_pentest/runner.py, api_pentest/scenarios/base_scenario.py</files>
  <action>
**In `api_pentest/runner.py`:**

1. Add import at top: `from api_pentest.core.response_patterns import ResponsePatternLearner`
2. In `PentestRunner.__init__()`, add: `self.response_learner: ResponsePatternLearner | None = None`
3. In `PentestRunner.run()`, AFTER `self.init_http()` and BEFORE the scenario loop, add the learning pre-pass:
   ```python
   # Learn response patterns before running scenarios
   self.response_learner = ResponsePatternLearner(
       http_client=self.http,
       endpoints=self.endpoints,
       oauth_handler=self.oauth_a,
   )
   self.response_learner.learn()
   ```
4. In the scenario setup call inside the `for sid in selected:` loop, add `response_learner=self.response_learner` as a new keyword argument to `scenario.setup()`.

**In `api_pentest/scenarios/base_scenario.py`:**

1. Add import: `from api_pentest.core.response_patterns import ResponsePatternLearner`
2. In `BaseScenario.__init__()`, add: `self.response_learner: ResponsePatternLearner | None = None`
3. In `BaseScenario.setup()` signature, add parameter: `response_learner: ResponsePatternLearner | None = None`
4. In `BaseScenario.setup()` body, add: `self.response_learner = response_learner`
5. Add new method `is_real_success()` to BaseScenario:
   ```python
   def is_real_success(self, evidence: Evidence, endpoint: Endpoint | None = None) -> bool:
       """Check if response represents genuine application-level success.

       Uses learned response patterns when available, falls back to
       HTTP status code check (is_success_status) when no learner or
       no pattern exists for the endpoint.
       """
       if not self.is_success_status(evidence.response_status):
           return False
       if self.response_learner is None:
           return True
       # Build endpoint key matching the learner's convention
       endpoint_key = f"{evidence.request_method}:{evidence.request_url}"
       return self.response_learner.is_real_success(evidence, endpoint_key)
   ```

   The method takes Evidence (always available at call sites) and optionally an Endpoint. It builds the endpoint_key from the evidence's request_method and request_url, which is what the HTTP client already records. This way callers don't need to construct the key themselves.

IMPORTANT: Keep `is_success_status()` method unchanged -- it is still used by scenarios for cases where HTTP-status-only checking is intentionally desired (e.g., baseline checks where we want to know if the HTTP request itself succeeded).

Do NOT change any scenario files in this task -- that is Plan 02's job.
  </action>
  <verify>
    python -c "
from api_pentest.scenarios.base_scenario import BaseScenario
assert hasattr(BaseScenario, 'is_real_success'), 'is_real_success not found'
assert hasattr(BaseScenario, 'is_success_status'), 'is_success_status should still exist'
print('BaseScenario OK')

from api_pentest.runner import PentestRunner
import inspect
src = inspect.getsource(PentestRunner.run)
assert 'response_learner' in src, 'response_learner not in run()'
print('Runner OK')
"
  </verify>
  <done>
    PentestRunner.run() creates ResponsePatternLearner and calls learn() before scenario loop.
    PentestRunner passes response_learner to each scenario.setup().
    BaseScenario.setup() accepts and stores response_learner parameter.
    BaseScenario.is_real_success() method exists and delegates to learner.
    BaseScenario.is_success_status() still exists unchanged for backward compatibility.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from api_pentest.core.response_patterns import ResponsePatternLearner, ResponsePattern"` -- imports without error
2. `python -c "from api_pentest.scenarios.base_scenario import BaseScenario; b = BaseScenario.__new__(BaseScenario); assert hasattr(b, 'is_real_success')"` -- method exists
3. `python -c "from api_pentest.runner import PentestRunner; import inspect; assert 'response_learner' in inspect.getsource(PentestRunner.run)"` -- runner integration present
4. All existing scenario imports still work (no broken imports): `python -c "import api_pentest.scenarios.s06_privileged_access; import api_pentest.scenarios.s09_business_flow; import api_pentest.scenarios.s13_unsafe_consumption; print('All imports OK')"`
</verification>

<success_criteria>
- ResponsePatternLearner exists and can be imported
- Runner calls learner.learn() before scenario execution
- BaseScenario has is_real_success() method that checks learned patterns
- is_success_status() is preserved (no regression for scenarios not yet updated)
- No new external dependencies introduced
</success_criteria>

<output>
After completion, create `.planning/phases/02-response-pattern-learning/02-01-SUMMARY.md`
</output>
