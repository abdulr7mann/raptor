---
phase: 07-advanced-validation-confidence
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - api_pentest/core/models.py
  - api_pentest/core/baseline_comparator.py
  - api_pentest/core/finding_validator.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "ConfidenceLevel enum exists with CONFIRMED, LIKELY, UNCERTAIN values"
    - "Finding model includes confidence, confidence_signals, and confidence_explanation fields"
    - "BaselineComparator detects meaningful differences in JSON structures while ignoring dynamic fields"
    - "FindingValidator collects 4 validation signals and determines confidence level"
  artifacts:
    - path: "api_pentest/core/models.py"
      provides: "ConfidenceLevel enum and extended Finding dataclass"
      contains: "class ConfidenceLevel"
    - path: "api_pentest/core/baseline_comparator.py"
      provides: "Structural diff with dynamic field exclusion"
      exports: ["BaselineComparator"]
    - path: "api_pentest/core/finding_validator.py"
      provides: "Multi-signal validation and confidence classification"
      exports: ["FindingValidator", "ValidationResult"]
  key_links:
    - from: "api_pentest/core/finding_validator.py"
      to: "api_pentest/core/baseline_comparator.py"
      via: "import BaselineComparator"
      pattern: "from.*baseline_comparator.*import.*BaselineComparator"
    - from: "api_pentest/core/finding_validator.py"
      to: "api_pentest/core/models.py"
      via: "import ConfidenceLevel, Finding, Evidence"
      pattern: "from.*models.*import.*ConfidenceLevel"
---

<objective>
Build the validation infrastructure for multi-signal confidence classification.

Purpose: Create the core components needed for validating findings against baselines
and classifying them with confidence levels (CONFIRMED, LIKELY, UNCERTAIN).

Output:
- ConfidenceLevel enum and extended Finding model
- BaselineComparator for structural JSON diff with dynamic field exclusion
- FindingValidator for multi-signal analysis and confidence determination
</objective>

<execution_context>
@/home/abdulr7man/.claude/get-shit-done/workflows/execute-plan.md
@/home/abdulr7man/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-advanced-validation-confidence/07-CONTEXT.md
@.planning/phases/07-advanced-validation-confidence/07-RESEARCH.md
@api_pentest/core/models.py
@api_pentest/core/response_patterns.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add ConfidenceLevel enum and extend Finding model</name>
  <files>api_pentest/core/models.py</files>
  <action>
    Add ConfidenceLevel enum after Severity enum with three values:
    - CONFIRMED = "CONFIRMED" (2+ validation signals)
    - LIKELY = "LIKELY" (1 validation signal)
    - UNCERTAIN = "UNCERTAIN" (0 validation signals)

    Extend Finding dataclass with three new fields (with defaults for backward compatibility):
    - confidence: ConfidenceLevel = ConfidenceLevel.UNCERTAIN
    - confidence_signals: list[str] = field(default_factory=list)
    - confidence_explanation: str = ""

    Update Finding.to_dict() to include the new fields:
    - "confidence": self.confidence.value
    - "confidence_signals": self.confidence_signals
    - "confidence_explanation": self.confidence_explanation
  </action>
  <verify>
    Run: python -c "from api_pentest.core.models import ConfidenceLevel, Finding; f = Finding(severity=Finding.__dataclass_fields__['severity'].type.INFO, title='test', description='desc'); print(f.confidence.value, f.confidence_signals, f.confidence_explanation)"
    Expected output: UNCERTAIN [] (empty string implied)
  </verify>
  <done>
    - ConfidenceLevel enum has CONFIRMED, LIKELY, UNCERTAIN values
    - Finding dataclass has confidence, confidence_signals, confidence_explanation fields
    - Finding.to_dict() includes all new fields
    - Default confidence is UNCERTAIN
  </done>
</task>

<task type="auto">
  <name>Task 2: Build BaselineComparator and FindingValidator</name>
  <files>
    api_pentest/core/baseline_comparator.py
    api_pentest/core/finding_validator.py
    requirements.txt
  </files>
  <action>
    **requirements.txt:** Add deepdiff>=8.0.0

    **baseline_comparator.py:** Create BaselineComparator class:
    - DYNAMIC_VALUE_PATTERNS: list of compiled regexes for:
      - ISO timestamps: r'^\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}'
      - UUIDs: r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$' (case insensitive)
      - Unix timestamps: r'^\d{10,13}$'
    - _is_dynamic_value(self, obj, path: str) -> bool: callback for DeepDiff
    - has_meaningful_diff(self, baseline_body: str, test_body: str) -> bool:
      Uses DeepDiff with ignore_order=True, exclude_obj_callback=_is_dynamic_value
      Falls back to string comparison for non-JSON bodies
    - has_structure_change(self, baseline_body: str, test_body: str) -> bool:
      Checks if top-level keys differ (added/removed keys, not value changes)
    - _parse_json(self, body: str) -> dict | None: safe JSON parser

    **finding_validator.py:** Create FindingValidator class:
    - ERROR_INDICATORS: set of lowercase strings like "error", "exception", "traceback", "stack trace", "syntax error", "internal server error", "fatal", "panic"
    - TIMING_MULTIPLIER = 3.0 (test > 3x baseline = anomaly)
    - __init__(self, baselines: dict[str, Evidence] | None = None):
      Store baselines dict keyed as "{method}:{url}"
      Create BaselineComparator instance
    - validate(self, finding: Finding, endpoint_key: str) -> Finding:
      Look up baseline by endpoint_key
      Collect signals, determine confidence, set explanation
      Return enriched Finding
    - _collect_signals(self, evidence: Evidence, baseline: Evidence | None) -> list[str]:
      Check each signal in order:
      1. "body_diff" - comparator.has_meaningful_diff()
      2. "timing_anomaly" - _has_timing_anomaly()
      3. "error_message" - _has_error_message()
      4. "structure_change" - comparator.has_structure_change()
      Return list of detected signal names
    - _has_timing_anomaly(self, evidence: Evidence, baseline: Evidence | None) -> bool:
      Return True if baseline exists and test time > baseline time * TIMING_MULTIPLIER
    - _has_error_message(self, evidence: Evidence, baseline: Evidence | None) -> bool:
      Check for error indicators in response body that weren't in baseline
    - _determine_confidence(self, signals: list[str]) -> ConfidenceLevel:
      len >= 2 -> CONFIRMED, len == 1 -> LIKELY, else -> UNCERTAIN
    - _build_explanation(self, signals: list[str], baseline_available: bool) -> str:
      Build human-readable explanation like "Validated by: response body diff, timing anomaly"
      or "No baseline available for comparison" if baseline was None

    **ValidationResult dataclass** (optional, in finding_validator.py):
    - signals_detected: list[str]
    - confidence: ConfidenceLevel
    - explanation: str
  </action>
  <verify>
    Run: pip install deepdiff>=8.0.0 && python -c "
from api_pentest.core.baseline_comparator import BaselineComparator
from api_pentest.core.finding_validator import FindingValidator
from api_pentest.core.models import Evidence, Finding, Severity, ConfidenceLevel

# Test comparator
comp = BaselineComparator()
assert comp.has_meaningful_diff('{\"a\": 1}', '{\"a\": 2}') == True
assert comp.has_meaningful_diff('{\"a\": 1}', '{\"a\": 1}') == False

# Test validator
validator = FindingValidator()
f = Finding(severity=Severity.MEDIUM, title='test', description='desc')
f.evidence = Evidence(response_body='{\"error\": \"fail\"}', response_time_ms=100)
result = validator.validate(f, 'GET:/test')
print('Confidence:', result.confidence.value)
print('Signals:', result.confidence_signals)
"
  </verify>
  <done>
    - deepdiff added to requirements.txt
    - BaselineComparator detects meaningful JSON diffs, ignoring dynamic values
    - BaselineComparator detects structural changes (key additions/removals)
    - FindingValidator collects up to 4 signals
    - FindingValidator classifies: 2+ signals = CONFIRMED, 1 = LIKELY, 0 = UNCERTAIN
    - FindingValidator builds human-readable explanations
  </done>
</task>

</tasks>

<verification>
1. Import test: `python -c "from api_pentest.core.models import ConfidenceLevel, Finding; from api_pentest.core.finding_validator import FindingValidator; from api_pentest.core.baseline_comparator import BaselineComparator; print('OK')"`
2. Finding backward compatibility: `python -c "from api_pentest.core.models import Finding, Severity; f = Finding(Severity.INFO, 'test', 'desc'); assert f.confidence.value == 'UNCERTAIN'"`
3. DeepDiff integration: `python -c "from api_pentest.core.baseline_comparator import BaselineComparator; c = BaselineComparator(); assert c.has_meaningful_diff('{\"id\": 1}', '{\"id\": 2}')"`
</verification>

<success_criteria>
- ConfidenceLevel enum exists with CONFIRMED, LIKELY, UNCERTAIN
- Finding model extended with confidence fields, backward compatible
- BaselineComparator uses deepdiff for structural comparison
- FindingValidator implements 4-signal validation with categorical threshold
- All imports resolve without error
</success_criteria>

<output>
After completion, create `.planning/phases/07-advanced-validation-confidence/07-01-SUMMARY.md`
</output>
