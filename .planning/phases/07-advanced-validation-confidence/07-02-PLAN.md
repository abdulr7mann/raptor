---
phase: 07-advanced-validation-confidence
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - api_pentest/scenarios/base_scenario.py
  - api_pentest/runner.py
  - api_pentest/core/response_patterns.py
autonomous: true

must_haves:
  truths:
    - "ResponsePatternLearner stores baseline Evidence objects alongside patterns"
    - "BaseScenario.log_finding() calls FindingValidator when available"
    - "Runner passes baselines to FindingValidator and injects it into scenarios"
    - "Findings logged after validation have appropriate confidence levels"
  artifacts:
    - path: "api_pentest/core/response_patterns.py"
      provides: "Baseline Evidence storage during learning"
      contains: "self.baselines"
    - path: "api_pentest/scenarios/base_scenario.py"
      provides: "Validation-aware finding creation"
      contains: "finding_validator"
    - path: "api_pentest/runner.py"
      provides: "FindingValidator instantiation and injection"
      contains: "FindingValidator"
  key_links:
    - from: "api_pentest/runner.py"
      to: "api_pentest/core/finding_validator.py"
      via: "import and instantiate"
      pattern: "from.*finding_validator.*import.*FindingValidator"
    - from: "api_pentest/scenarios/base_scenario.py"
      to: "api_pentest/core/finding_validator.py"
      via: "validator.validate call in log_finding"
      pattern: "self.finding_validator.validate"
---

<objective>
Wire FindingValidator into the scenario execution pipeline so all findings are
automatically validated and enriched with confidence levels.

Purpose: Integrate the validation infrastructure built in Plan 01 into the
existing runner and scenario workflow. Findings should be validated against
baselines stored during the learning phase.

Output:
- ResponsePatternLearner stores baseline Evidence for validator use
- BaseScenario.log_finding() validates findings when validator is available
- Runner creates FindingValidator with baselines and passes to scenarios
</objective>

<execution_context>
@/home/abdulr7man/.claude/get-shit-done/workflows/execute-plan.md
@/home/abdulr7man/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-advanced-validation-confidence/07-CONTEXT.md
@.planning/phases/07-advanced-validation-confidence/07-01-SUMMARY.md
@api_pentest/core/response_patterns.py
@api_pentest/scenarios/base_scenario.py
@api_pentest/runner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend ResponsePatternLearner to store baseline Evidence</name>
  <files>api_pentest/core/response_patterns.py</files>
  <action>
    Extend ResponsePatternLearner to store the Evidence objects collected during learning:

    1. Add `self.baselines: dict[str, Evidence] = {}` in __init__()

    2. In the learn() method, after each successful probe:
       - Store success_evidence (if exists) in self.baselines[endpoint_key]
       - If no success_evidence, store failure_evidence as fallback baseline
       - Log baseline storage at debug level

    3. Add property `def get_baselines(self) -> dict[str, Evidence]:` that returns
       self.baselines (convenience accessor for runner)

    This allows FindingValidator to access baseline Evidence captured during the
    learning phase without making additional requests.
  </action>
  <verify>
    Run: python -c "
from api_pentest.core.response_patterns import ResponsePatternLearner
from api_pentest.core.http_client import PentestHttpClient

# Mock minimal test
http = PentestHttpClient(timeout=5)
learner = ResponsePatternLearner(http, [], None)
# Check baselines dict exists
assert hasattr(learner, 'baselines')
assert isinstance(learner.baselines, dict)
print('Baselines storage OK')
"
  </verify>
  <done>
    - ResponsePatternLearner has self.baselines dict initialized in __init__
    - learn() stores Evidence objects keyed by endpoint
    - get_baselines() property returns the baselines dict
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire FindingValidator into BaseScenario and Runner</name>
  <files>
    api_pentest/scenarios/base_scenario.py
    api_pentest/runner.py
  </files>
  <action>
    **base_scenario.py:**

    1. Add import at top:
       `from api_pentest.core.finding_validator import FindingValidator`

    2. In __init__(), add:
       `self.finding_validator: FindingValidator | None = None`

    3. In setup(), add parameter:
       `finding_validator: FindingValidator | None = None`
       And set: `self.finding_validator = finding_validator`

    4. Modify log_finding() to validate findings when validator is available:
       ```python
       def log_finding(
           self,
           severity: Severity,
           title: str,
           description: str,
           endpoint: str = "",
           evidence: Evidence | None = None,
           remediation: str = "",
       ):
           """Record a security finding with confidence validation."""
           finding = Finding(
               severity=severity,
               title=title,
               description=description,
               endpoint=endpoint,
               evidence=evidence,
               remediation=remediation,
               owasp_id=self.OWASP_ID,
               owasp_name=self.OWASP_NAME,
               scenario_id=self.SCENARIO_ID,
           )

           # Validate and enrich with confidence if validator available
           if self.finding_validator and evidence:
               endpoint_key = f"{evidence.request_method}:{evidence.request_url}"
               finding = self.finding_validator.validate(finding, endpoint_key)

           self.findings.append(finding)
       ```

    **runner.py:**

    1. Add import at top:
       `from api_pentest.core.finding_validator import FindingValidator`

    2. In __init__(), add:
       `self.finding_validator: FindingValidator | None = None`

    3. In run(), after response_learner.learn() completes, create FindingValidator:
       ```python
       # Create finding validator with learned baselines
       baselines = self.response_learner.baselines if self.response_learner else {}
       self.finding_validator = FindingValidator(baselines=baselines)
       ```

    4. In the scenario.setup() call, pass the validator:
       ```python
       scenario.setup(
           endpoints=endpoints_for_scenario,
           oauth_handler=self.oauth_a,
           http_client=self.http,
           config=self.config,
           oauth_handler_b=self.oauth_b,
           response_learner=self.response_learner,
           prerequisite_results=self.prerequisite_results,
           api_profile=self.api_profile,
           finding_validator=self.finding_validator,  # NEW
       )
       ```
  </action>
  <verify>
    Run: python -c "
from api_pentest.scenarios.base_scenario import BaseScenario
from api_pentest.core.finding_validator import FindingValidator
from api_pentest.core.models import Severity, Evidence

# Check validator can be set on scenario
class TestScenario(BaseScenario):
    SCENARIO_ID = 'test'
    SCENARIO_NAME = 'Test'
    def get_test_cases(self): return []
    def execute_test(self, test_name): pass

scenario = TestScenario()
validator = FindingValidator()
scenario.finding_validator = validator
assert scenario.finding_validator is not None
print('Validator wiring OK')
"
  </verify>
  <done>
    - BaseScenario accepts and stores finding_validator
    - BaseScenario.log_finding() validates findings when validator and evidence are present
    - Runner creates FindingValidator with baselines from response_learner
    - Runner passes finding_validator to scenario.setup()
  </done>
</task>

</tasks>

<verification>
1. Import chain: `python -c "from api_pentest.runner import PentestRunner; from api_pentest.scenarios.base_scenario import BaseScenario; print('OK')"`
2. Validator injection: Check that runner.py contains `finding_validator=self.finding_validator` in setup call
3. Baselines storage: Check response_patterns.py contains `self.baselines` initialization
</verification>

<success_criteria>
- ResponsePatternLearner stores Evidence objects during learning
- BaseScenario.log_finding() enriches findings with confidence when validator present
- Runner creates FindingValidator with learned baselines and passes to scenarios
- Findings flow through validation before being appended
</success_criteria>

<output>
After completion, create `.planning/phases/07-advanced-validation-confidence/07-02-SUMMARY.md`
</output>
