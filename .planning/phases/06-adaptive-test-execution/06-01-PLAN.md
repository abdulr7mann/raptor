---
phase: 06-adaptive-test-execution
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - api_pentest/core/models.py
  - api_pentest/core/response_formats.py
  - api_pentest/core/relevance.py
autonomous: true

must_haves:
  truths:
    - "ScenarioApplicability dataclass exists for declaring test requirements"
    - "ResponseFormatHandler safely parses JSON, XML, and plain text responses"
    - "RelevanceCalculator computes weighted scores from architecture, classification, and prerequisites"
  artifacts:
    - path: "api_pentest/core/models.py"
      provides: "ScenarioApplicability, ApplicabilityMode"
      contains: "class ScenarioApplicability"
    - path: "api_pentest/core/response_formats.py"
      provides: "ResponseFormatHandler with JSON/XML/text parsing"
      contains: "class ResponseFormatHandler"
    - path: "api_pentest/core/relevance.py"
      provides: "RelevanceCalculator, RelevanceScore"
      contains: "class RelevanceCalculator"
  key_links:
    - from: "api_pentest/core/relevance.py"
      to: "api_pentest/core/api_discovery.py"
      via: "import ArchitectureType"
      pattern: "from api_pentest.core.api_discovery import ArchitectureType"
    - from: "api_pentest/core/response_formats.py"
      to: "defusedxml"
      via: "import for safe XML parsing"
      pattern: "import defusedxml"
---

<objective>
Build the core infrastructure for adaptive test execution: ScenarioApplicability declarations, response format handling, and relevance scoring.

Purpose: Enable intelligent test selection based on API profile. Scenarios will declare their applicability (architecture, classification requirements), the relevance calculator will score each test-endpoint pair, and the response format handler will safely parse diverse response types.

Output: Three new modules ready for integration into scenarios and runner.
</objective>

<execution_context>
@/home/abdulr7man/.claude/get-shit-done/workflows/execute-plan.md
@/home/abdulr7man/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-adaptive-test-execution/06-CONTEXT.md
@.planning/phases/06-adaptive-test-execution/06-RESEARCH.md
@api_pentest/core/models.py
@api_pentest/core/api_discovery.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add ScenarioApplicability dataclass and ApplicabilityMode enum to models.py</name>
  <files>api_pentest/core/models.py</files>
  <action>
    Add two new constructs to models.py:

    1. **ApplicabilityMode enum** with three values:
       - ANY: Applies if ANY listed value matches
       - ALL: Applies only if ALL listed values match
       - EXCLUDE: Applies if NONE of the listed values match

    2. **ScenarioApplicability dataclass** with fields:
       - architectures: list[ArchitectureType] (empty = applies to all)
       - architecture_mode: ApplicabilityMode (default ANY)
       - classifications: list[str] (values: "public", "protected", "auth-endpoint"; empty = applies to all)
       - classification_mode: ApplicabilityMode (default ANY)
       - requires_prerequisites: list[str] (e.g., ["rate_limiting"]; empty = no prereq requirements)

    Import ArchitectureType from api_discovery module.

    Place these after the existing EndpointClassification enum, before the Endpoint dataclass.

    Follow existing code style: dataclass pattern, field(default_factory=list) for lists.
  </action>
  <verify>
    Run: python -c "from api_pentest.core.models import ScenarioApplicability, ApplicabilityMode; print(ScenarioApplicability())"

    Expected: ScenarioApplicability(architectures=[], architecture_mode=<ApplicabilityMode.ANY: ...>, classifications=[], classification_mode=<ApplicabilityMode.ANY: ...>, requires_prerequisites=[])
  </verify>
  <done>
    ScenarioApplicability dataclass importable from models.py with correct defaults.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ResponseFormatHandler for safe JSON/XML/text parsing</name>
  <files>api_pentest/core/response_formats.py</files>
  <action>
    Create new module api_pentest/core/response_formats.py with:

    1. **ResponseFormatHandler class** with:
       - JSON_TYPES frozenset: {"application/json", "application/vnd.api+json", "application/hal+json", "application/problem+json", "text/json"}
       - XML_TYPES frozenset: {"application/xml", "text/xml", "application/soap+xml", "application/xhtml+xml", "application/atom+xml", "application/rss+xml"}

       - `detect_content_type(evidence) -> str`: Extract base Content-Type from response headers (strip charset, etc.)

       - `parse(evidence) -> tuple[Any, str]`: Parse response body based on Content-Type.
         Returns (parsed_data, format_type) where format_type is "json", "xml", "text", or "empty".
         - JSON: use json.loads(), catch JSONDecodeError
         - XML: use defusedxml.ElementTree.fromstring(), catch ParseError
         - Fallback: return raw body as text

       - `parse_json_safe(evidence) -> dict | list | None`: Convenience method returning parsed JSON or None.

    Use defusedxml for XML parsing (protects against XXE attacks).

    Add logging at DEBUG level for parse failures.

    Follow existing module patterns: module docstring, logger = logging.getLogger(__name__).
  </action>
  <verify>
    Run: python -c "from api_pentest.core.response_formats import ResponseFormatHandler; h = ResponseFormatHandler(); print(h.JSON_TYPES)"

    Expected: frozenset containing 'application/json' and other JSON types.
  </verify>
  <done>
    ResponseFormatHandler class parses JSON, XML, and text with graceful fallback. defusedxml used for safe XML parsing.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create RelevanceCalculator with weighted scoring</name>
  <files>api_pentest/core/relevance.py</files>
  <action>
    Create new module api_pentest/core/relevance.py with:

    1. **RelevanceScore dataclass** with fields:
       - total: float (0.0 to 1.0)
       - architecture_score: float
       - classification_score: float
       - prerequisite_score: float
       - skip_reason: str (empty if above threshold)

    2. **RelevanceCalculator class** with:
       - Class constants:
         - WEIGHT_ARCHITECTURE = 0.4
         - WEIGHT_CLASSIFICATION = 0.3
         - WEIGHT_PREREQUISITE = 0.3

       - `__init__(api_profile, prerequisite_results: dict, threshold: float = 0.3)`:
         Store profile, prereqs, and threshold.

       - `calculate(scenario_applicability: ScenarioApplicability, endpoint: Endpoint) -> RelevanceScore`:
         Compute weighted score:

         **Architecture scoring:**
         - If applicability.architectures is empty: full score (0.4)
         - If profile.architecture_type matches any in list: full score (0.4)
         - Else: score 0.0, set skip_reason

         **Classification scoring:**
         - If applicability.classifications is empty: full score (0.3)
         - If endpoint.classification.value matches any in list: full score (0.3)
         - Else: score 0.0, set skip_reason

         **Prerequisite scoring:**
         - If applicability.requires_prerequisites is empty: full score (0.3)
         - If all required prereqs have status PRESENT: full score (0.3)
         - Else: score 0.0, set skip_reason

         Total = sum of three scores. If total < threshold, ensure skip_reason is set.

    Import ArchitectureType from api_discovery, ScenarioApplicability from models, DetectionStatus from prerequisite_detector.

    Follow existing patterns: dataclass, type hints, logging.
  </action>
  <verify>
    Run: python -c "
from api_pentest.core.relevance import RelevanceCalculator, RelevanceScore
from api_pentest.core.models import ScenarioApplicability, Endpoint
# Create minimal test
score = RelevanceScore(total=0.7, architecture_score=0.4, classification_score=0.3, prerequisite_score=0.0, skip_reason='')
print(f'Score: {score.total}')
"

    Expected: Score: 0.7
  </verify>
  <done>
    RelevanceCalculator computes weighted scores using three factors (0.4 + 0.3 + 0.3), returns RelevanceScore with breakdown and skip_reason.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Import all new constructs:
   ```bash
   python -c "
   from api_pentest.core.models import ScenarioApplicability, ApplicabilityMode
   from api_pentest.core.response_formats import ResponseFormatHandler
   from api_pentest.core.relevance import RelevanceCalculator, RelevanceScore
   print('All imports successful')
   "
   ```

2. Verify defusedxml is available (or add to requirements):
   ```bash
   pip show defusedxml || pip install defusedxml>=0.7.1
   ```

3. Quick integration test:
   ```bash
   python -c "
   from api_pentest.core.models import ScenarioApplicability, ApplicabilityMode
   from api_pentest.core.api_discovery import ArchitectureType
   app = ScenarioApplicability(architectures=[ArchitectureType.REST])
   print(f'Applicability for REST: {app}')
   "
   ```
</verification>

<success_criteria>
- ScenarioApplicability and ApplicabilityMode importable from models.py
- ResponseFormatHandler class parses JSON, XML, text safely with defusedxml
- RelevanceCalculator produces weighted scores with correct formula (0.4 + 0.3 + 0.3)
- All modules follow existing codebase conventions
- No import cycles introduced
</success_criteria>

<output>
After completion, create `.planning/phases/06-adaptive-test-execution/06-01-SUMMARY.md`
</output>
