---
phase: 06-adaptive-test-execution
plan: 03
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - api_pentest/runner.py
  - run_pentest.py
autonomous: true

must_haves:
  truths:
    - "Runner filters scenarios based on architecture applicability before execution"
    - "Skipped scenarios and tests log their skip reason at INFO level"
    - "CLI supports --relevance-threshold and --fast flags"
    - "Tests below relevance threshold are skipped with logged reason"
    - "Each test-endpoint pair has relevance score calculated via RelevanceCalculator.calculate() (TEST-04)"
    - "Endpoints below threshold are filtered out before scenario executes (TEST-04)"
  artifacts:
    - path: "api_pentest/runner.py"
      provides: "Applicability filtering, relevance threshold integration, per-test relevance scoring"
      contains: "RelevanceCalculator"
    - path: "run_pentest.py"
      provides: "CLI flags for relevance threshold and fast mode"
      contains: "--relevance-threshold"
  key_links:
    - from: "api_pentest/runner.py"
      to: "api_pentest/core/relevance.py"
      via: "import RelevanceCalculator"
      pattern: "from api_pentest.core.relevance import RelevanceCalculator"
    - from: "api_pentest/runner.py"
      to: "api_pentest/core/relevance.py"
      via: "calls relevance_calculator.calculate() for each test-endpoint pair (TEST-04)"
      pattern: "self\\.relevance_calculator\\.calculate\\("
    - from: "run_pentest.py"
      to: "api_pentest/runner.py"
      via: "passes relevance_threshold to config"
      pattern: "relevance_threshold"
---

<objective>
Wire applicability filtering and relevance scoring into the runner, add CLI support for relevance threshold configuration.

Purpose: Make the runner intelligent about which scenarios to run. Before executing a scenario, check if it's applicable to the detected API architecture. Pass the api_profile to scenarios so they can access discovered characteristics.

Output: Runner performs architecture-based scenario filtering, CLI supports --relevance-threshold and --fast flags.
</objective>

<execution_context>
@/home/abdulr7man/.claude/get-shit-done/workflows/execute-plan.md
@/home/abdulr7man/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-adaptive-test-execution/06-CONTEXT.md
@.planning/phases/06-adaptive-test-execution/06-RESEARCH.md
@.planning/phases/06-adaptive-test-execution/06-01-SUMMARY.md
@api_pentest/runner.py
@run_pentest.py
@api_pentest/core/relevance.py
@api_pentest/core/api_discovery.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add applicability filtering and api_profile passing to runner.py</name>
  <files>api_pentest/runner.py</files>
  <action>
    Update runner.py to filter scenarios based on applicability and pass api_profile to scenarios:

    1. **Add imports** at top of file:
       ```python
       from api_pentest.core.models import ScenarioApplicability
       from api_pentest.core.api_discovery import ArchitectureType
       from api_pentest.core.relevance import RelevanceCalculator
       ```

    2. **In run() method, after scenario class is loaded** (around line 230), add architecture filtering:
       ```python
       # Check scenario-level applicability against API profile
       applicability = getattr(scenario_class, 'APPLICABILITY', ScenarioApplicability())

       # Architecture filtering: skip if scenario doesn't apply to detected architecture
       if applicability.architectures and self.api_profile:
           try:
               profile_arch = ArchitectureType(self.api_profile.architecture_type)
           except ValueError:
               profile_arch = ArchitectureType.UNKNOWN

           if profile_arch not in applicability.architectures:
               logger.info(
                   "Skipping %s: architecture %s not in %s",
                   sid.upper(),
                   profile_arch.value,
                   [a.value for a in applicability.architectures],
               )
               print(f"{Fore.CYAN}[{sid.upper()}] Skipped: architecture {profile_arch.value} not applicable{Style.RESET_ALL}")
               continue
       ```

    3. **Update scenario.setup() call** to pass api_profile:
       ```python
       scenario.setup(
           endpoints=self.endpoints,
           oauth_handler=self.oauth_a,
           http_client=self.http,
           config=self.config,
           oauth_handler_b=self.oauth_b,
           response_learner=self.response_learner,
           prerequisite_results=self.prerequisite_results,
           api_profile=self.api_profile,  # NEW: pass profile to scenario
       )
       ```

    4. **Add relevance_threshold to config processing** in run() method, after api_profile is set:
       ```python
       # Build relevance calculator for scenarios to use
       relevance_threshold = self.config.get("relevance_threshold", 0.3)
       self.relevance_calculator = RelevanceCalculator(
           api_profile=self.api_profile,
           prerequisite_results=self.prerequisite_results,
           threshold=relevance_threshold,
       )
       ```

    5. **Add relevance_calculator attribute** in __init__:
       ```python
       self.relevance_calculator = None
       ```

    6. **CRITICAL (TEST-04): Add per-test relevance filtering loop AFTER scenario applicability check**
       Within the scenario execution loop, AFTER architecture filtering passes, add test-level relevance checking:
       ```python
       # Per-test relevance filtering (TEST-04)
       # For each endpoint the scenario will test, check relevance score
       if self.relevance_calculator and self.endpoints:
           skipped_tests = []
           relevant_endpoints = []

           for endpoint in self.endpoints:
               # Calculate relevance score for this test-endpoint pair
               # calculate() signature: (scenario_applicability, endpoint) -> RelevanceScore
               score = self.relevance_calculator.calculate(applicability, endpoint)

               if score.total < relevance_threshold:
                   skipped_tests.append((endpoint.get("path", "unknown"), score.total))
                   logger.info(
                       "Skipping test %s on %s: relevance score %.2f < threshold %.2f",
                       sid.upper(),
                       endpoint.get("path", "unknown"),
                       score.total,
                       relevance_threshold,
                   )
               else:
                   relevant_endpoints.append(endpoint)

           # Log skipped tests if any
           if skipped_tests:
               for path, score_val in skipped_tests[:3]:  # Limit output
                   print(f"  {Fore.YELLOW}[{sid.upper()}] Skipped {path}: score {score_val:.2f} < {relevance_threshold}{Style.RESET_ALL}")
               if len(skipped_tests) > 3:
                   print(f"  {Fore.YELLOW}  ... and {len(skipped_tests) - 3} more{Style.RESET_ALL}")

           # If no endpoints are relevant, skip the entire scenario
           if not relevant_endpoints:
               logger.info(
                   "Skipping %s entirely: no endpoints above relevance threshold %.2f",
                   sid.upper(),
                   relevance_threshold,
               )
               skipped_scenarios.append((sid, f"no endpoints above threshold {relevance_threshold}"))
               continue

           # Replace endpoints with filtered list for this scenario
           # Store original and restore after scenario completes
           original_endpoints = self.endpoints
           self.endpoints = relevant_endpoints
       ```

       Add at the end of the scenario's execution (after findings processing):
       ```python
       # Restore original endpoints if we filtered
       if self.relevance_calculator and 'original_endpoints' in dir():
           self.endpoints = original_endpoints
       ```
  </action>
  <verify>
    Run: python -c "
from api_pentest.runner import PentestRunner
r = PentestRunner({})
print('relevance_calculator attr:', hasattr(r, 'relevance_calculator'))
print('Runner imports successful')
"

    Expected: Shows relevance_calculator attr: True and success message.
  </verify>
  <done>
    Runner filters scenarios by architecture applicability, passes api_profile to scenarios, initializes RelevanceCalculator, and performs per-test relevance filtering by calling relevance_calculator.calculate() for each test-endpoint pair (TEST-04).
  </done>
</task>

<task type="auto">
  <name>Task 2: Add --relevance-threshold and --fast CLI flags to run_pentest.py</name>
  <files>run_pentest.py</files>
  <action>
    Update run_pentest.py to support relevance threshold configuration:

    1. **Add new arguments** after --verbose:
       ```python
       parser.add_argument(
           "--relevance-threshold",
           type=float,
           default=0.3,
           metavar="SCORE",
           help="Minimum relevance score (0.0-1.0) to run a test. "
                "Lower = more tests, higher coverage. "
                "Higher = faster, fewer tests. Default: 0.3",
       )
       parser.add_argument(
           "--fast",
           action="store_true",
           help="Fast mode: raise relevance threshold to 0.6 for quicker scans "
                "(reduces test coverage)",
       )
       ```

    2. **Update config processing** after `config = load_config(...)`:
       ```python
       # Apply relevance threshold from CLI
       if args.relevance_threshold:
           config["relevance_threshold"] = args.relevance_threshold

       # Fast mode overrides threshold
       if args.fast:
           config["relevance_threshold"] = max(0.6, config.get("relevance_threshold", 0.3))
           print(f"Fast mode enabled: relevance threshold set to {config['relevance_threshold']}")
       ```

    3. **Validate threshold range** after config processing:
       ```python
       threshold = config.get("relevance_threshold", 0.3)
       if not 0.0 <= threshold <= 1.0:
           print(f"Error: relevance-threshold must be between 0.0 and 1.0", file=sys.stderr)
           sys.exit(1)
       ```
  </action>
  <verify>
    Run: python run_pentest.py --help | grep -A2 "relevance-threshold"

    Expected: Shows --relevance-threshold argument with description.
  </verify>
  <done>
    CLI supports --relevance-threshold (0.0-1.0) and --fast flags for controlling test selection.
  </done>
</task>

<task type="auto">
  <name>Task 3: Print skip summary and threshold info in runner output</name>
  <files>api_pentest/runner.py</files>
  <action>
    Add informative output about filtering and thresholds:

    1. **After scenarios are filtered** (before the main loop), add summary:
       ```python
       # Print relevance threshold info
       relevance_threshold = self.config.get("relevance_threshold", 0.3)
       print(f"  Relevance threshold: {relevance_threshold}")
       ```
       (Add this after the "Endpoints: {ep_count}" line in the discovery summary section)

    2. **Track skipped scenarios** - add counter before the scenario loop:
       ```python
       skipped_scenarios = []
       ```

    3. **Record skipped scenarios** in the architecture filter block:
       ```python
       skipped_scenarios.append((sid, f"architecture {profile_arch.value} not applicable"))
       ```

    4. **Add skip summary after main loop** (before _print_summary):
       ```python
       # Print skipped scenario summary if any
       if skipped_scenarios:
           print(f"\n{Fore.CYAN}Scenarios skipped due to applicability:{Style.RESET_ALL}")
           for sid, reason in skipped_scenarios:
               print(f"  {sid.upper()}: {reason}")
       ```
  </action>
  <verify>
    Run: python -c "
# Just verify the import works and constants are accessible
from api_pentest.runner import PentestRunner
print('Runner modifications successful')
"

    Expected: Success message.
  </verify>
  <done>
    Runner prints relevance threshold, tracks skipped scenarios, and shows skip summary with reasons.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Verify CLI flags work:
   ```bash
   python run_pentest.py --help 2>&1 | grep -E "(relevance|fast)"
   ```

2. Verify threshold validation:
   ```bash
   python run_pentest.py --input /dev/null --relevance-threshold 1.5 2>&1 | grep -i "error"
   # Should show error about invalid range (but will fail on missing file first)
   ```

3. Test imports and runner initialization:
   ```bash
   python -c "
   from api_pentest.runner import PentestRunner, SCENARIO_MODULES
   from api_pentest.core.relevance import RelevanceCalculator
   r = PentestRunner({'relevance_threshold': 0.5})
   print('Configured threshold:', r.config.get('relevance_threshold'))
   print('Scenario count:', len(SCENARIO_MODULES))
   "
   ```

4. Verify fast mode:
   ```bash
   python -c "
   import sys
   sys.argv = ['test', '--input', 'test.json', '--fast']
   # Can't fully test without file, but verify arg parsing works
   import argparse
   parser = argparse.ArgumentParser()
   parser.add_argument('--input', '-i', required=True)
   parser.add_argument('--fast', action='store_true')
   parser.add_argument('--relevance-threshold', type=float, default=0.3)
   args = parser.parse_args()
   print(f'Fast mode: {args.fast}')
   "
   ```
</verification>

<success_criteria>
- Runner imports RelevanceCalculator and ScenarioApplicability without errors
- Scenarios are filtered by architecture before execution
- Skipped scenarios are logged with reason
- scenario.setup() receives api_profile parameter
- CLI accepts --relevance-threshold with float value
- CLI accepts --fast flag that sets threshold to 0.6
- Invalid threshold values are rejected
- Skip summary printed after scenario execution
- RelevanceCalculator.calculate() is called for each test-endpoint pair in the scenario loop (TEST-04)
- Endpoints with relevance score below threshold are filtered out before scenario runs (TEST-04)
- Skipped test-endpoint pairs are logged with their score and reason (TEST-04)
</success_criteria>

<output>
After completion, create `.planning/phases/06-adaptive-test-execution/06-03-SUMMARY.md`
</output>
